{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extra Credit\n",
    "-  Preprocess the Negations in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umair Ahmad\n",
    "# 21i-2081\n",
    "## Umair.Ahmad@Nu.edu.pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment #5\n",
    "# Fake News Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from urduhack.preprocessing import normalize_whitespace\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data,stop_words):\n",
    "    negations=[\"نہیں\",\"نا\",\"نہ\"]\n",
    "    nlp = spacy.blank('ur')\n",
    "    doc = nlp(data)\n",
    "    cleaned_data =[]\n",
    "    for index, x in enumerate(doc):\n",
    "        if  len(doc)-1>index:\n",
    "            if str(x) not in stop_words and str(doc[index+1])not in negations:\n",
    "                if str(x) not in negations:\n",
    "                    cleaned_data.append(str(x))\n",
    "            \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "Stop_Words=open('C:/Users/Umair/Downloads/NLP assignmnent 5/stopwords-ur.txt', encoding='utf-8').read()\n",
    "Stop_Words=Stop_Words.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Features Extraction\n",
    "- Function \"get_data\" recive the three arguments Path of for filereading\n",
    "-  Pass list of Stop word if you want to remove stopwords otherwise pass the empty string\n",
    "-  Set BN to True if you want to remove the duplicates from each features otherwise set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path,stop_words,BN):\n",
    "    sentences=[]\n",
    "    if(\".\" not in path):\n",
    "        dir_names=listdir(path)\n",
    "        for file in dir_names:\n",
    "            doc=open(str(path)+\"/\"+str(file), encoding='utf-8').read()\n",
    "            lst=data_cleaning(doc,stop_words)\n",
    "            if BN == True:\n",
    "                lst = list(set(lst))\n",
    "            sentences=sentences+lst\n",
    "    else:\n",
    "        doc=open(path, encoding='utf-8').read()\n",
    "        lst=data_cleaning(doc,stop_words)\n",
    "        if BN == True:\n",
    "            lst = list(set(lst))\n",
    "        sentences=sentences+lst\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparations for Multinomial NB With Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50597\n",
      "77863\n"
     ]
    }
   ],
   "source": [
    "MN_Fake_Data=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Fake',Stop_Words,False)\n",
    "MN_Real_Data=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Real',Stop_Words,False)\n",
    "print(len(MN_Fake_Data))\n",
    "print(len(MN_Real_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparations for Boolean NB With Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33199\n",
      "46872\n"
     ]
    }
   ],
   "source": [
    "BN_Fake_Data=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Fake',Stop_Words,True)\n",
    "BN_Real_Data=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Real',Stop_Words,True)\n",
    "print(len(BN_Fake_Data))\n",
    "print(len(BN_Real_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparations for MultiNomial NB Without Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87423\n",
      "130810\n"
     ]
    }
   ],
   "source": [
    "MN_Fake_Data_with_Stopwords=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Fake',\"\",False)\n",
    "MN_Real_Data_with_Stopwords=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Real',\"\",False)\n",
    "print(len(MN_Fake_Data_with_Stopwords))\n",
    "print(len(MN_Real_Data_with_Stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparations for Boolean NB Without Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44494\n",
      "61104\n"
     ]
    }
   ],
   "source": [
    "BN_Fake_Data_with_Stopwords=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Fake',\"\",True)\n",
    "BN_Real_Data_with_Stopwords=get_data('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Real',\"\",True)\n",
    "print(len(BN_Fake_Data_with_Stopwords))\n",
    "print(len(BN_Real_Data_with_Stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Calculation for Fake and Real Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45141065830721006\n",
      "0.54858934169279\n"
     ]
    }
   ],
   "source": [
    "Fake_train_dir=listdir('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Fake')\n",
    "Real_train_dir=listdir('C:/Users/Umair/Downloads/NLP assignmnent 5/Train/Real')\n",
    "Prior_of_Train_Fake=len(Fake_train_dir)/(len(Fake_train_dir)+len(Real_train_dir))\n",
    "Prior_of_Train_Real=len(Real_train_dir)/(len(Fake_train_dir)+len(Real_train_dir))\n",
    "print(Prior_of_Train_Fake)\n",
    "print(Prior_of_Train_Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning of Features\n",
    "- \"Traning\" function receive the two arguments Fake Data and Real Data \n",
    "- Returns the Tranined Conditional Probabilities of the given data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traning(Fake_data,Real_data):\n",
    "    wordsFreq_fake = defaultdict(float)\n",
    "    wordsFreq_real = defaultdict(float)\n",
    "    cond_prob_fake=defaultdict(float)\n",
    "    cond_prob_real=defaultdict(float)\n",
    "    Vocab=defaultdict(float)\n",
    "    for word in Fake_data:\n",
    "        wordsFreq_fake[str(word)]+=1\n",
    "        Vocab[str(word)]+=1\n",
    "    for word in Real_data:\n",
    "        wordsFreq_real[str(word)]+=1\n",
    "        Vocab[str(word)]+=1\n",
    "    for key,value in Vocab.items():\n",
    "        cond_prob_fake[str(key)]=(wordsFreq_fake[str(key)]+1)/(len(Fake_data)+len(Vocab))\n",
    "        cond_prob_real[str(key)]=(wordsFreq_real[key]+1)/(len(Real_data)+len(Vocab))\n",
    "    return cond_prob_real,cond_prob_fake\n",
    "\n",
    "MN_cond_prob_real,MN_cond_prob_fake=Traning(MN_Fake_Data,MN_Real_Data)\n",
    "BN_cond_prob_real,BN_cond_prob_fake=Traning(BN_Fake_Data,BN_Real_Data)\n",
    "MN_cond_prob_real_SW,MN_cond_prob_fake_SW=Traning(MN_Fake_Data_with_Stopwords,MN_Real_Data_with_Stopwords)\n",
    "BN_cond_prob_real_SW,BN_cond_prob_fake_SW=Traning(BN_Fake_Data_with_Stopwords,BN_Real_Data_with_Stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of TestData\n",
    "- Function \"Testing\" receive the Conditional probabilties and Prior of the both classes \n",
    "- Return the Predicited Label from log probabilties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing(cond_prob_real,cond_prob_fake,Prior_of_Fake,Prior_of_Real,text):\n",
    "    score={\"Real\":0,\"Fake\":0}\n",
    "    score[\"Real\"]=math.log(Prior_of_Real)\n",
    "    score[\"Fake\"]=math.log(Prior_of_Fake)\n",
    "    for x in text:\n",
    "        if cond_prob_real[str(x)]>0:\n",
    "            score[\"Real\"]+=math.log(cond_prob_real[str(x)])\n",
    "        if cond_prob_fake[str(x)]>0:\n",
    "            score[\"Fake\"]+=math.log(cond_prob_fake[str(x)])\n",
    "    if score[\"Real\"]>score[\"Fake\"]:\n",
    "        return \"Real\"\n",
    "    else:\n",
    "        return \"Fake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Labels of all the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fake_path='C:/Users/Umair/Downloads/NLP assignmnent 5/Test/Fake'\n",
    "Real_path='C:/Users/Umair/Downloads/NLP assignmnent 5/Test/Real'\n",
    "Fake_dir_names=listdir(Fake_path)\n",
    "Real_dir_names=listdir(Real_path)\n",
    "\n",
    "def cal_labels(real_cond,fake_cond,fake_prior,real_prior,stop_words,BN):\n",
    "    Ypre=[]\n",
    "    for x in Fake_dir_names:\n",
    "        text=get_data(Fake_path+\"/\"+x,stop_words,BN)\n",
    "        ypred=Testing(real_cond,fake_cond,fake_prior,real_prior,text)\n",
    "        Ypre.append(ypred) \n",
    "    for x in Real_dir_names:\n",
    "        text=get_data(Real_path+\"/\"+x,stop_words,BN)\n",
    "        ypred=Testing(real_cond,fake_cond,fake_prior,real_prior,text)\n",
    "        Ypre.append(ypred)\n",
    "    return Ypre\n",
    "\n",
    "MN_Ypre=cal_labels(MN_cond_prob_real,MN_cond_prob_fake,Prior_of_Train_Fake,Prior_of_Train_Real,Stop_Words,False)\n",
    "BN_Ypre=cal_labels(BN_cond_prob_real,BN_cond_prob_fake,Prior_of_Train_Fake,Prior_of_Train_Real,Stop_Words,True)\n",
    "MN_Ypre_SW=cal_labels(MN_cond_prob_real_SW,MN_cond_prob_fake_SW,Prior_of_Train_Fake,Prior_of_Train_Real,\"\",False)\n",
    "BN_Ypre_SW=cal_labels(BN_cond_prob_real_SW,BN_cond_prob_fake_SW,Prior_of_Train_Fake,Prior_of_Train_Real,\"\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Anaylsis\n",
    "There are sometimes news that use words with negation. We Preprocess the negations in the dataset provide the Result.\n",
    "After seeing the results of \n",
    "- Na¨ıve Bayes classifier\n",
    "- Boolean Na¨ıve Bayes classifier\n",
    "- Experiment with/without using stop words\n",
    "\n",
    "Metrics for evaluating performance of this classifier are precision,recall, f1-score and accuracy.\n",
    "After Traning of a Na¨ıve Bayes classifier on the data set provided on all four combination mentioned above.\n",
    "These trained model is tested on the testset of Fake and Real new.\n",
    "\n",
    "- Train Dataset(Real=350,Fake=288)\n",
    "- Test Dataset (Real=150,Fake=262)\n",
    "\n",
    "All the classification report are shown below also the average accuracy score of all the four trained models are\n",
    "\n",
    "- **Accuracy Score of Multinomial NB With    Stop Words:    0.70**\n",
    "- **Accuracy Score of Boolean     NB With    Stop Words:    0.78**\n",
    "- **Accuracy Score of Multinomial NB Without Stop Words:    0.73**\n",
    "- **Accuracy Score of Boolean     NB Without Stop Words:    0.79**\n",
    "\n",
    "Rest of the Evaluation measures Precision,Recall and F1 measures are shown below\n",
    "\n",
    "## Conculsion \n",
    "We can achive good results by choosing Boolean Naive Bayes without stopwords also by handling the negations, it can further get better if we identify the nouns, adjctives, verbs attached with negations by POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report of Accuracy, Precision, Recall and F1 Measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6870229007633588\n",
      "0.7557251908396947\n",
      "0.7480916030534351\n",
      "0.7862595419847328\n"
     ]
    }
   ],
   "source": [
    "y_Real = [\"Real\"] * len(Real_dir_names)\n",
    "y_Fake = [\"Fake\"] * len(Fake_dir_names)\n",
    "Ytest=y_Fake+y_Real\n",
    "MN_acc=accuracy_score(Ytest, MN_Ypre)\n",
    "BN_acc=accuracy_score(Ytest, BN_Ypre)\n",
    "MN_acc_SW=accuracy_score(Ytest, MN_Ypre_SW)\n",
    "BN_acc_SW=accuracy_score(Ytest, BN_Ypre_SW)\n",
    "print(MN_acc)\n",
    "print(BN_acc)\n",
    "print(MN_acc_SW)\n",
    "print(BN_acc_SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.65      0.59      0.62       112\n",
      "        Real       0.71      0.76      0.74       150\n",
      "\n",
      "    accuracy                           0.69       262\n",
      "   macro avg       0.68      0.67      0.68       262\n",
      "weighted avg       0.68      0.69      0.68       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = [\"Fake\", \"Real\"]\n",
    "print(classification_report(Ytest, MN_Ypre, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.77      0.61      0.68       112\n",
      "        Real       0.75      0.87      0.80       150\n",
      "\n",
      "    accuracy                           0.76       262\n",
      "   macro avg       0.76      0.74      0.74       262\n",
      "weighted avg       0.76      0.76      0.75       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest, BN_Ypre, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.72      0.68      0.70       112\n",
      "        Real       0.77      0.80      0.78       150\n",
      "\n",
      "    accuracy                           0.75       262\n",
      "   macro avg       0.74      0.74      0.74       262\n",
      "weighted avg       0.75      0.75      0.75       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest, MN_Ypre_SW, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.80      0.67      0.73       112\n",
      "        Real       0.78      0.87      0.82       150\n",
      "\n",
      "    accuracy                           0.79       262\n",
      "   macro avg       0.79      0.77      0.78       262\n",
      "weighted avg       0.79      0.79      0.78       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest, BN_Ypre_SW, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
